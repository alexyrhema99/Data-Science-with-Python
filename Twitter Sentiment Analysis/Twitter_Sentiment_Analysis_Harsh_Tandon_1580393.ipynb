{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis\n",
    "#### Author: Harsh Tandon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the part 1 of our homework, we will be analyzing the sentiment of tweets about **'Trump'** and in part 2 we will be analyzing the sentiment of tweets about Indian Prime Minister **'Modi'**.<br>\n",
    "We begin with getting data from twitter using Twitter API. We will compare the compare the words in the tweets with a list of positive and negative words to evaluate the sentiment. We will also be accounting for stop words, and whichever words do not fall under these categories will be classified as 'others'.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and set up jupyter interactivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A regular expression is a special sequence of characters that helps you match or find other strings or sets of strings, using a specialized syntax held in a pattern. Regular expressions are widely used in UNIX world. The Python module re provides full support for Perl-like regular expressions in Python.<br> We will be using regular expression for cleaninig the tweets data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for regular expressions \n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print all outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Sentiment Analysis for Trump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be reading our data extracted from twitter API. The extracted file is a .txt file.<br>\n",
    "Next, we create an empty list called **'tweets'** and append each tweet from the file seperated by a '\\n' return command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile = open(\"Trump.txt\") #open the file containing all tweets\n",
    "tweets = [] #creating an empty list\n",
    "\n",
    "#extracting each line from txt file and appending it in the list\n",
    "for line in myfile: \n",
    "        for j in line.split(r\"\\n\"):\n",
    "            tweets.append(j)\n",
    "myfile.close() #close the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b\\'RT @TeaPainUSA: Trump on Lev Parnas (just now): \"I don\\\\\\'t even know who this man is.\"',\n",
       " '',\n",
       " 'This will bite him.',\n",
       " '\\'b\\'TIL, Trump fans don\\\\\\'t know how to use \"sit this one out,\" but they_really_want to',\n",
       " \"'b'Pelosi Loses It On House Floor, Compares Trump To Murdering Mobster https://t.co/J3zMEsaoqs via @realmattcouch\",\n",
       " \"'b'Nancy Pelosi is eaten alive with jealousy, rage , hatred. While her constituents sink in piles of feces and needles\\\\xe2\\\\x80\\\\xa6 https://t.co/4U9KfqNjbC\",\n",
       " '\\'b\\'RT @davecclarke: Quite the @nytimes review for \"A Very Stable Genius\" by @PhilipRucker and @CarolLeonnig \"They\\\\xe2\\\\x80\\\\x99re meticulous journalists, a\\\\xe2\\\\x80\\\\xa6',\n",
       " '\\'b\"RT @BernieSanders: Not only will we reverse the damage Trump has done to workers\\' rights, we will expand those rights and double union memb\\\\xe2\\\\x80\\\\xa6',\n",
       " '\"b\\'RT @SJW_ForAll: The members of this jury who are @GOP members holding their hands up taking an oath, swearing to be impartial is an absolut\\\\xe2\\\\x80\\\\xa6',\n",
       " \"'b'RT @joncoopertweets: As @SpeakerPelosi correctly points out, @Facebook is in bed with Trump. They\\\\xe2\\\\x80\\\\x99re going to do everything they can to hel\\\\xe2\\\\x80\\\\xa6\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[:10] #lets print and see how our created list looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks dirty right? Let's clean it with the help of regular expressions.<br>\n",
    "Well run a loop for to access each tweet in our list of tweets and if the length of that tweet is greater than 1, we will use regular expression on it. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets = [] #create an empty list which will contain cleaned tweets\n",
    "for tweet in tweets:\n",
    "    if tweet is not None:\n",
    "        tweet_parse = tweet.split(':')\n",
    "        if len(tweet_parse) > 1:                               #if the length of tweet is greater than 1, proceed with cleaning\n",
    "            tweet = tweet_parse[-1]\n",
    "            tweet = re.sub(r'\\\\x[0-9a-f][0-9a-f]', \"\", tweet)  #to deal with emoticons. Hex code for emoticons start with x\n",
    "            tweet = re.sub('@(\\w)+', \"\", tweet)                #to deal with mentions and usernames\n",
    "            tweet = re.sub('&amp', \"\", tweet)                  #to deal with extra white spaces\n",
    "            tweet = re.sub('//t.co/(\\w)+', \"\",tweet)           #to deal with hyperlinks\n",
    "            tweet = re.sub('//t.c', \"\",tweet)                  #to deal with hyperlinks\n",
    "            tweet = re.sub(r'\\\\',\"\", tweet)                    #to deal with double backward slashes\n",
    "            tweet = re.sub('#',\"\", tweet)\n",
    "            tweet = re.sub(\"\\'\", \" \" ,tweet)\n",
    "            tweet = tweet.replace(\",\",\"\")\n",
    "            tweet = tweet.replace(\"*\",\"\")\n",
    "            tweet = tweet.replace(\"//t\",\"\")\n",
    "            tweet = tweet.replace('\"',\"\")\n",
    "            tweet = tweet.replace(\".\",\"\")\n",
    "            clean_tweets.append(tweet.lower())                #after cleaning each tweet is appended to clean_tweets list\n",
    "\n",
    "# Create a list of words from cleansed tweets\n",
    "clean_tweet_words = [] #create an empty list for cleaned tweet words\n",
    "for i in clean_tweets:\n",
    "    s = i.split()\n",
    "    for j in s:\n",
    "        clean_tweet_words.append(j) #append cleaned words to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'don', 't', 'even', 'know', 'who', 'this', 'man', 'is', 'via']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tweet_words[:10]#lets have a look at the list of cleaned words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is positive, negative and stop words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive into analyzing our list of clean words from the tweets, we need something to compare them to.<br>\n",
    "We read three different text files, each containing a variety to stop words, positive words and negative words.<br>\n",
    "We will create three empty lists and append each each category of word into their respective lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = [] #an empty list which will hold stop words\n",
    "positive = [] #an empty list which will hold positive words\n",
    "negative = [] #an empty list which will hold negative words\n",
    "\n",
    "#read all the stop words and append each of them in the stopWords list\n",
    "with open('stopwords.txt', 'r') as f:\n",
    "    for word in f:\n",
    "        word = word.split('\\n')\n",
    "        stopWords.append(word[0])\n",
    "        \n",
    "#read all the postive words and append each of them in the positive list\n",
    "with open('positive.txt', 'r') as f:\n",
    "    for word in f:\n",
    "        word = word.split('\\n')\n",
    "        positive.append(word[0])\n",
    "\n",
    "#read all the negative words and append each of them in the negative list\n",
    "with open('negative.txt', 'r') as f:\n",
    "    for word in f:\n",
    "        word = word.split('\\n')\n",
    "        negative.append(word[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '!!', '?!', '??', '!?', '`', '``', \"''\", '-lrb-', '-rrb-']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopWords[:10] #lets have a look at the stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a+',\n",
       " 'abound',\n",
       " 'abounds',\n",
       " 'abundance',\n",
       " 'abundant',\n",
       " 'accessable',\n",
       " 'accessible',\n",
       " 'acclaim',\n",
       " 'acclaimed',\n",
       " 'acclamation']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive[:10] #lets have a look at the positive list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2-faced',\n",
       " '2-faces',\n",
       " 'abnormal',\n",
       " 'abolish',\n",
       " 'abominable',\n",
       " 'abominably',\n",
       " 'abominate',\n",
       " 'abomination',\n",
       " 'abort',\n",
       " 'aborted']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative[:10] #lets have a look at the negative list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets Analyze!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below defines 4 different count variables, each assigned to zero.<br>\n",
    "There is another count variable named 'sentiment' which is incremented every time a positive word pops up in our tweets, and decremented every time a negative word pops up. In simpler terms, this variable gives the difference between the number of positive and negative words in our tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopCount = 0 #count variable for stopWords\n",
    "posCount = 0 #count variable for positive words\n",
    "negCount = 0 #count variable for negative words\n",
    "otherCount = 0 #count variable for 'others'\n",
    "sentiment = 0 #count variable to calculate overall sentiment\n",
    "\n",
    "#compare each word from the tweet to each of the lists\n",
    "for word in clean_tweet_words:\n",
    "    if word in stopWords:\n",
    "        stopCount += 1\n",
    "    elif word in positive:\n",
    "        posCount += 1\n",
    "        sentiment += 1\n",
    "    elif word in negative:\n",
    "        negCount += 1\n",
    "        sentiment -= 1\n",
    "    else:\n",
    "        otherCount += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the percentage of total number of positive, negative, stop words and others by dividing the count with total number of tweeted words. <br>\n",
    "The overall sentiment is calculated by checking the sentiment score. Remember sentiment score is the difference between the positive and negative words in the tweets. If the sentiment score is positive, it means the overall sentiment is positive. If the sentiment score is negative, it means the overall sentiment is negative. <br>\n",
    "After this we check whether it is a strong sentiment or not. For this we divide sentiment by total positive and negative words (not all words). This gives us the percentage difference between positive and negative words alone. If the percentage difference between the sentiments is greater than 50%, the sentiment is strong, otherwise, it is a weak sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In all the tweets:\n",
      "\tTotal number of positive words are : 2564\n",
      "\tTotal number of negative words are : 2112\n",
      "\tTotal number of stop words are : 28302\n",
      "\tTotal number of 'other' words are : 22036\n",
      "\n",
      "The ratios(percentages) are as follows:\n",
      "\tPositive: 4.66 %\n",
      "\tNegative: 3.84%\n",
      "\tStopWords: 51.45%\n",
      "\tOthers: 40.06%\n",
      "\n",
      "Checking whether overall sentiment is postive or negative:\n",
      "Sentiment score :  452\n",
      "\n",
      "Overall the sentiment is positive\n",
      "Percentage difference between positive and negative words 9.67%\n",
      "The sentiment is weak as it is less than 50% of the tweets\n"
     ]
    }
   ],
   "source": [
    "print(\"In all the tweets:\\n\\tTotal number of positive words are : \" + str(posCount))\n",
    "print(\"\\tTotal number of negative words are : \"  + str(negCount))\n",
    "print(\"\\tTotal number of stop words are : \" + str(stopCount))\n",
    "print(\"\\tTotal number of 'other' words are : \" + str(otherCount))\n",
    "        \n",
    "overallPos = (posCount/len(clean_tweet_words))*100\n",
    "overallNeg = (negCount/len(clean_tweet_words))*100\n",
    "overallStop = (stopCount/len(clean_tweet_words))*100\n",
    "overallOther = (otherCount/len(clean_tweet_words))*100\n",
    "\n",
    "print(\"\\nThe ratios(percentages) are as follows:\\n\\tPositive: %.2f \" % overallPos + \"%\")\n",
    "print(\"\\tNegative: %.2f\" %overallNeg + \"%\")\n",
    "print(\"\\tStopWords: %.2f\" %overallStop+ \"%\")\n",
    "print(\"\\tOthers: %.2f\" %overallOther+ \"%\")\n",
    "\n",
    "#checking whether positive or negative is greater \n",
    "print (\"\\nChecking whether overall sentiment is postive or negative:\")\n",
    "\n",
    "print(\"Sentiment score : \", sentiment)\n",
    "print (\"\\nOverall the sentiment is positive\") if sentiment >= 0 else print (\"Overall sentiment is negative \")\n",
    "\n",
    "#checking if the sentiment is strong or not\n",
    "print(\"Percentage difference between positive and negative words %.2f\" % ((100*sentiment)/(posCount+negCount))+\"%\")\n",
    "print(\"The sentiment is weak as it is less than 50% of the tweets\") if ((100*sentiment)/(posCount+negCount)) < 50 else print(\"The sentiment is strong as it is more than or equal to 50%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Sentiment Analysis for Modi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat all the steps from part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile = open(\"Modi.txt\") #open the file containing all tweets\n",
    "tweets = [] #creating an empty list\n",
    "\n",
    "#extracting each line from txt file and appending it in the list\n",
    "for line in myfile: \n",
    "        for j in line.split(r\"\\n\"):\n",
    "            tweets.append(j)\n",
    "myfile.close() #close the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"b'RT @SaketGokhale: Piyush Goyal is miffed at Jeff Bezos purely because he can\\\\xe2\\\\x80\\\\x99t call &amp; bully him for censoring news about Modi in the Washin\\\\xe2\\\\x80\\\\xa6\",\n",
       " '\\'b\"RT @ashoswai: #MyPiece - Not for destroying India\\'s secularism but his failure on the economic front which has made Modi vulnerable. So, th\\\\xe2\\\\x80\\\\xa6',\n",
       " '\"b\"RT @TheMamuns: @htTweets But, no CHAUKEEDAARI. ',\n",
       " \"Because, under the Modi's guidance.\",\n",
       " 'Group of CHAUKEEDAAR criminal mandates.',\n",
       " 'Today the India\\\\xe2\\\\x80\\\\xa6',\n",
       " '\"b\"RT @ajitdatta: BJP\\'s opponents have often questioned PM Modi\\'s legitimacy to rule on the ground that over 60% of the electorate did not vot\\\\xe2\\\\x80\\\\xa6',\n",
       " '\"b\\'RT @NekkantiR: Modi has shown us, the sleeping beauties Hindus, how \"chilled out\" we are living among burkaSecularists  aka ChuslamiTerrori\\\\xe2\\\\x80\\\\xa6',\n",
       " '\\'b\"RT @CTR_Nirmalkumar: Dear My PM Modi Ji,',\n",
       " '']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[:10] #lets print and see how our created list looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets = [] #create an empty list which will contain cleaned tweets\n",
    "for tweet in tweets:\n",
    "    if tweet is not None:\n",
    "        tweet_parse = tweet.split(':')\n",
    "        if len(tweet_parse) > 1:                               #if the length of tweet is greater than 1, proceed with cleaning\n",
    "            tweet = tweet_parse[-1]\n",
    "            tweet = re.sub(r'\\\\x[0-9a-f][0-9a-f]', \"\", tweet)  #to deal with emoticons. Hex code for emoticons start with x\n",
    "            tweet = re.sub('@(\\w)+', \"\", tweet)                #to deal with mentions and usernames\n",
    "            tweet = re.sub('&amp', \"\", tweet)                  #to deal with extra white spaces\n",
    "            tweet = re.sub('//t.co/(\\w)+', \"\",tweet)           #to deal with hyperlinks\n",
    "            tweet = re.sub('//t.c', \"\",tweet)                  #to deal with hyperlinks\n",
    "            tweet = re.sub(r'\\\\',\"\", tweet)                    #to deal with double backward slashes\n",
    "            tweet = re.sub('#',\"\", tweet)\n",
    "            tweet = re.sub(\"\\'\", \" \" ,tweet)\n",
    "            tweet = tweet.replace(\",\",\"\")\n",
    "            tweet = tweet.replace(\"*\",\"\")\n",
    "            tweet = tweet.replace(\"//t\",\"\")\n",
    "            tweet = tweet.replace('\"',\"\")\n",
    "            tweet = tweet.replace(\".\",\"\")\n",
    "            clean_tweets.append(tweet.lower())                #after cleaning each tweet is appended to clean_tweets list\n",
    "\n",
    "# Create a list of words from cleansed tweets\n",
    "clean_tweet_words = [] #create an empty list for cleaned tweet words\n",
    "for i in clean_tweets:\n",
    "    s = i.split()\n",
    "    for j in s:\n",
    "        clean_tweet_words.append(j) #append cleaned words to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['piyush',\n",
       " 'goyal',\n",
       " 'is',\n",
       " 'miffed',\n",
       " 'at',\n",
       " 'jeff',\n",
       " 'bezos',\n",
       " 'purely',\n",
       " 'because',\n",
       " 'he']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tweet_words[:10] #lets have a look at the list of cleaned words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already read and stored the positive, negative and stop word file. No need to repeat that step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopCount = 0 #count variable for stopWords\n",
    "posCount = 0 #count variable for positive words\n",
    "negCount = 0 #count variable for negative words\n",
    "otherCount = 0 #count variable for 'others'\n",
    "sentiment = 0 #count variable to calculate overall sentiment\n",
    "\n",
    "#compare each word from the tweet to each of the lists\n",
    "for word in clean_tweet_words:\n",
    "    if word in stopWords:\n",
    "        stopCount += 1\n",
    "    elif word in positive:\n",
    "        posCount += 1\n",
    "        sentiment += 1\n",
    "    elif word in negative:\n",
    "        negCount += 1\n",
    "        sentiment -= 1\n",
    "    else:\n",
    "        otherCount += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In all the tweets:\n",
      "\tTotal number of positive words are : 630\n",
      "\tTotal number of negative words are : 1294\n",
      "\tTotal number of stop words are : 21387\n",
      "\tTotal number of 'other' words are : 19728\n",
      "\n",
      "The ratios(percentages) are as follows:\n",
      "\tPositive: 1.46 %\n",
      "\tNegative: 3.01%\n",
      "\tStopWords: 49.69%\n",
      "\tOthers: 45.84%\n",
      "\n",
      "Checking whether overall sentiment is postive or negative:\n",
      "Sentiment score :  -664\n",
      "Overall sentiment is negative \n",
      "Percentage difference between positive and negative words -34.51%\n",
      "The sentiment is weak as it is less than 50% of the tweets\n"
     ]
    }
   ],
   "source": [
    "print(\"In all the tweets:\\n\\tTotal number of positive words are : \" + str(posCount))\n",
    "print(\"\\tTotal number of negative words are : \"  + str(negCount))\n",
    "print(\"\\tTotal number of stop words are : \" + str(stopCount))\n",
    "print(\"\\tTotal number of 'other' words are : \" + str(otherCount))\n",
    "        \n",
    "overallPos = (posCount/len(clean_tweet_words))*100\n",
    "overallNeg = (negCount/len(clean_tweet_words))*100\n",
    "overallStop = (stopCount/len(clean_tweet_words))*100\n",
    "overallOther = (otherCount/len(clean_tweet_words))*100\n",
    "\n",
    "print(\"\\nThe ratios(percentages) are as follows:\\n\\tPositive: %.2f \" % overallPos + \"%\")\n",
    "print(\"\\tNegative: %.2f\" %overallNeg + \"%\")\n",
    "print(\"\\tStopWords: %.2f\" %overallStop+ \"%\")\n",
    "print(\"\\tOthers: %.2f\" %overallOther+ \"%\")\n",
    "\n",
    "#checking whether positive or negative is greater \n",
    "print (\"\\nChecking whether overall sentiment is postive or negative:\")\n",
    "\n",
    "print(\"Sentiment score : \", sentiment)\n",
    "print (\"\\nOverall the sentiment is positive\") if sentiment >= 0 else print (\"Overall sentiment is negative \")\n",
    "\n",
    "#checking if the sentiment is strong or not\n",
    "print(\"Percentage difference between positive and negative words %.2f\" % ((100*sentiment)/(posCount+negCount))+\"%\")\n",
    "print(\"The sentiment is weak as it is less than 50% of the tweets\") if ((100*sentiment)/(posCount+negCount)) < 50 else print(\"The sentiment is strong as it is more than or equal to 50%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
